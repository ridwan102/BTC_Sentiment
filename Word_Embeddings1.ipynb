{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will get an overview of how to generate word vectors using the various word embedding methods discussed in the lecture\n",
    "\n",
    "### Objectives:\n",
    "- Implement Count Vectors with sklearn\n",
    "- Implement TF-IDF Vectors with sklearn and gensim\n",
    "- Train and save word2vec model with gensim\n",
    "- Load Google's pretrained word2vec model\n",
    "- Load Stanford's pretrained GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "corpus = twenty_train.data[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\nSubject: Converting images to HP LaserJet III?\nNntp-Posting-Host: hampton\nOrganization: The City University\nLines: 14\n\nDoes anyone know of a good way (standard PC application/PD utility) to\nconvert tif/img/tga files into LaserJet III format.  We would also like to\ndo the same, converting to HPGL (HP plotter) files.\n\nPlease email any response.\n\nIs this the correct group?\n\nThanks in advance.  Michael.\n-- \nMichael Collier (Programmer)                 The Computer Unit,\nEmail: M.P.Collier@uk.ac.city                The City University,\nTel: 071 477-8000 x3769                      London,\nFax: 071 477-8565                            EC1V 0HB.\n\n"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
     ]
    }
   ],
   "source": [
    "print(f\"{corpus[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "output_type": "stream",
     "name": "stdout",
=======
     "name": "stdout",
     "output_type": "stream",
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
     "text": [
      "Dimensions of Document-term matrix: (50, 3075)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(f\"Dimensions of Document-term matrix: {X.toarray().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see that the second dimension gives us the size of our vocabulary.\n",
    "\n",
    "But why restrict ourselves to single words? We can pass an additional argument to the CountVectorizer() object to add n-grams to our vocabulary.\n",
    "\n",
    "What is an n-gram?  It's just a collection of n consecutive words. For example:\n",
    "\"New\", \"York\", \"City\", \"subway\" are all unigrams\n",
    "\"New York\", \"York City\", \"City subway\" are bigrams\n",
    "\"New York City\", \"York City subway\" are trigrams\n",
    "\"New York City subway\" is a 4-gram\n",
    "\n",
    "We can specify to include n-grams with the ngram_range argument.  This takes a tuple which specifies the range of n-grams that we should include (inclusively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "output_type": "stream",
     "name": "stdout",
=======
     "name": "stdout",
     "output_type": "stream",
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
     "text": [
      "Dimensions of Document-term matrix: (50, 23397)\n"
     ]
    }
   ],
   "source": [
    "# Include unigrams, bigrams, and trigrams\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(f\"Dimensions of Document-term matrix: {X.toarray().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common preprocessing step in many NLP applications is stop-word removal.\n",
    "Common words like \"a\", \"the\", \"and\" often add a lot of noise, and don't typiccally contribute much to the task we are trying to solve.\n",
    "\n",
    "CountVectorizer also comes equipped with a way of dealing with common English stop words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "output_type": "stream",
     "name": "stdout",
=======
     "name": "stdout",
     "output_type": "stream",
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
     "text": [
      "Dimensions of Document-term matrix: (50, 14300)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3), stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(f\"Dimensions of Document-term matrix: {X.toarray().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectors\n",
    "\n",
    "Here we will demonstrate two ways to generate TF-IDF vectors with both sklearn and gensim.  It's good to be aware of both methods because depending on your specific workflow, one method might be easier than the other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sklearn, it's VERY similar to how we did CountVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CountVector: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 2 2 1 1 0 0 0 0 0 0 0]\n\n\nTFIDF: [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n 0.    0.    0.    0.127 0.127 0.063 0.063 0.    0.    0.    0.    0.\n 0.    0.   ]\n"
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVector: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 2 2 1 1 0 0 0 0 0 0 0]\n",
      "\n",
      "\n",
      "TFIDF: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.12664863 0.12664863 0.06332431\n",
      " 0.06332431 0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "import numpy as np\n",
=======
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    "# How do these two representations compare?\n",
    "# Let's look at the first 50 dimensions of the first document to gain some intuition\n",
    "\n",
    "#np.set_printoptions(precision=3) # This just makes things a little easier to read\n",
    "\n",
    "print(f\"CountVector: {X.toarray()[0,0:50]}\\n\\n\")\n",
    "\n",
    "print(f\"TFIDF: {X_tfidf.toarray()[0,0:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn isn't our only option for doing TF-IDF.  Gensim is another popular library for many NLP tasks"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 9,
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 10,
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents\n",
    "tokenized_docs = [gensim.utils.simple_preprocess(d) for d in corpus]\n",
    "\n",
    "# Create a Gensim Dictionary.  This creates an id to word mapping for everything in our vocbulary\n",
    "# It is NOT the same as the dictionary object in the Python standard library\n",
    "mydict = gensim.corpora.Dictionary()\n",
    "\n",
    "# Create a Gensim Corpus object.  This creates a list of tuples for each document.\n",
    "# The first element of the tuple is the word id, the second is the number of counts\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 11,
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the doc-term matrix as a numpy array.\n",
    "# Typically these matrices are HUGE so, it's usuall not a great idea to create the full dense doc-term matrix.\n",
    "# We do it here to illustrate that you can get the same info as we obtained in scikit-learn!\n",
    "doc_term_matrix = gensim.matutils.corpus2dense(mycorpus, num_terms=len(mydict))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
=======
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
     "data": {
      "text/plain": [
       "array([[2., 0., 0., ..., 2., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 3., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
<<<<<<< HEAD
     "metadata": {},
     "execution_count": 14
=======
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 13,
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tf-idf model is very simple!\n",
    "tfidf = gensim.models.TfidfModel(mycorpus)\n",
    "tfidf_matrix = gensim.matutils.corpus2dense(tfidf[mycorpus], num_terms=len(mydict))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.146, 0.   , 0.   , ..., 0.141, 0.   , 0.   ],\n",
       "       [0.093, 0.   , 0.   , ..., 0.   , 0.   , 0.   ],\n",
       "       [0.037, 0.   , 0.056, ..., 0.   , 0.   , 0.   ],\n",
       "       ...,\n",
       "       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.072],\n",
       "       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.072],\n",
       "       [0.   , 0.   , 0.   , ..., 0.   , 0.   , 0.072]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 16
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14608695, 0.        , 0.        , ..., 0.14052968, 0.        ,\n",
       "        0.        ],\n",
       "       [0.09308913, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.03681387, 0.        , 0.05624168, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.07188244],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.07188244],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.07188244]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and GloVe\n",
    "\n",
    "Word2Vec is a very powerful and useful word embedding method.  The math can get a little sticky, but luckily Gensim comes equipped with ways for us to train our own Word2Vec model, or load in a pre-trained word2vec model.  Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 15,
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    "\n",
    "tokenized_docs = [gensim.utils.simple_preprocess(d) for d in documents]\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 16,
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
   "metadata": {},
   "outputs": [],
   "source": [
    "# size refers to the desired dimension of our word vectors\n",
    "# window refers to the size of our context window\n",
    "# sg means that we are using the Skip-gram architecture\n",
    "\n",
    "model = gensim.models.Word2Vec(tokenized_docs, size=10, window=2,min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.011  0.032  0.003 -0.005 -0.013  0.01  -0.024  0.008 -0.036  0.007]\n"
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02408477  0.01935017 -0.00804691 -0.03953621 -0.04088669  0.01037319\n",
      " -0.04984108  0.00269309  0.04118545 -0.00026856]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
     ]
    }
   ],
   "source": [
    "print(model['human'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our own model with word2vec is pretty cool, but it requires us to have a large corpus of data.\n",
    "\n",
    "Fortunately, research groups at Stanford and Google have made their pre-trained word embeddings publicly available for us to use!\n",
    "\n",
    "Google's word2vec: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "GloVe:  https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Just note that these model's will require ~4 GB of RAM to fit in memory"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 21,
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to where the word2vec file lives\n",
<<<<<<< HEAD
    "google_vec_file = '/Users/ridwan/Documents/DataScience/Metis/Metis_Projects/Data/word_embeddings/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it!  This might take a few minutes...\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
=======
    "# google_vec_file = '/Users/adam/Downloads/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "model = api.load('word2vec-google-news-300')"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-3.320e-02, -4.980e-01,  6.128e-02,  3.301e-01,  2.197e-01,\n",
       "       -2.461e-01,  1.035e-01,  5.786e-02, -2.480e-01, -2.031e-01,\n",
       "        5.518e-02,  1.641e-01,  4.346e-02, -5.547e-01,  1.455e-01,\n",
       "       -1.328e-01,  2.266e-01,  3.711e-02, -4.663e-02, -1.436e-01,\n",
       "       -1.396e-01,  7.275e-02, -2.041e-01, -1.045e-01,  5.249e-02,\n",
       "        1.172e-01, -4.766e-01, -1.447e-02,  2.695e-01, -2.559e-01,\n",
       "       -5.493e-02,  2.412e-01, -1.621e-01,  1.514e-01,  9.717e-02,\n",
       "       -2.930e-01, -6.445e-02,  2.676e-01,  7.764e-02, -5.430e-01,\n",
       "        1.040e-01,  1.973e-01,  5.493e-02,  6.226e-02,  1.040e-01,\n",
       "       -3.496e-01, -1.953e-01,  1.406e-01, -5.798e-03, -1.270e-01,\n",
       "        4.175e-02, -3.438e-01, -3.262e-01,  6.226e-02, -6.016e-01,\n",
       "        2.217e-01, -2.871e-01, -2.715e-01, -1.504e-01, -3.691e-01,\n",
       "        1.426e-01,  2.324e-01,  3.760e-02,  2.930e-01, -3.848e-01,\n",
       "       -3.809e-02, -1.240e-01, -2.559e-01, -8.118e-03, -5.859e-02,\n",
       "       -1.147e-01,  2.520e-01, -4.053e-02, -2.520e-01,  2.031e-01,\n",
       "       -1.025e-01,  1.670e-01,  8.008e-02,  1.338e-01,  3.691e-01,\n",
       "       -3.633e-01, -4.468e-02, -9.082e-02,  2.148e-01,  3.906e-01,\n",
       "       -2.363e-01, -3.320e-01,  2.139e-01,  1.187e-01,  3.247e-02,\n",
       "       -7.959e-02, -6.592e-02, -1.147e-01, -2.451e-01, -3.340e-01,\n",
       "       -3.379e-01,  2.217e-01, -1.719e-01,  1.016e-01, -4.902e-01,\n",
       "        1.201e-01, -2.715e-01, -1.543e-01, -5.347e-02,  6.689e-02,\n",
       "       -2.676e-01,  1.689e-01, -4.028e-02, -8.789e-02,  1.260e-01,\n",
       "       -3.438e-01,  1.934e-01,  3.770e-01,  1.455e-01, -2.275e-01,\n",
       "       -1.689e-01,  1.855e-01, -1.309e-01, -2.041e-01, -1.631e-01,\n",
       "        3.711e-02,  2.461e-01, -1.836e-01, -2.559e-01,  3.398e-01,\n",
       "       -3.882e-02,  3.882e-02,  1.299e-01, -1.865e-01,  9.717e-02,\n",
       "        1.963e-01,  3.955e-02, -5.117e-01,  1.182e-01,  9.814e-02,\n",
       "        3.203e-01,  1.030e-01, -1.733e-02,  2.695e-01,  1.797e-01,\n",
       "        1.011e-01,  4.062e-01, -4.102e-02,  7.129e-02,  1.089e-01,\n",
       "        1.276e-02,  1.099e-02, -1.436e-01,  2.598e-01,  1.074e-01,\n",
       "        2.520e-01,  1.406e-01, -3.281e-01,  1.572e-01,  1.875e-01,\n",
       "       -1.426e-01,  1.992e-01,  1.289e-01,  6.226e-02, -2.930e-01,\n",
       "        1.147e-01,  1.392e-02, -8.350e-02, -3.008e-01,  2.676e-01,\n",
       "        8.740e-02, -1.387e-01,  3.672e-01, -1.636e-02,  2.520e-01,\n",
       "       -3.262e-01, -7.324e-02,  6.494e-02, -4.238e-01,  3.672e-01,\n",
       "        3.540e-03, -1.929e-02,  6.006e-02, -2.820e-02, -9.717e-02,\n",
       "        4.492e-02, -1.514e-01,  2.236e-01, -2.539e-01, -1.270e-01,\n",
       "       -1.436e-01, -5.054e-02, -4.077e-02, -3.711e-02,  1.602e-01,\n",
       "       -2.539e-01, -2.637e-01, -1.680e-01, -3.301e-01,  2.490e-01,\n",
       "       -4.062e-01, -5.640e-02,  2.207e-01,  2.637e-01,  3.564e-02,\n",
       "       -6.348e-02,  6.165e-03,  3.906e-01, -3.105e-01, -2.324e-01,\n",
       "        2.061e-01,  1.895e-01, -1.719e-01,  1.459e-02, -6.738e-02,\n",
       "        4.761e-02, -9.131e-02,  4.443e-02,  1.631e-01, -2.490e-01,\n",
       "        2.461e-01,  8.154e-02,  7.568e-02, -5.078e-02, -4.258e-01,\n",
       "       -2.812e-01,  1.680e-01, -3.613e-02, -8.008e-02, -1.641e-01,\n",
       "        6.934e-02,  3.203e-01, -4.126e-02,  1.672e-02,  1.904e-01,\n",
       "        3.379e-01, -3.066e-01, -5.273e-01,  1.602e-01, -2.656e-01,\n",
       "       -3.711e-01,  9.570e-02,  6.079e-02,  2.500e-01,  1.318e-01,\n",
       "        1.865e-01, -2.363e-01, -1.523e-01, -4.199e-01,  1.064e-01,\n",
       "       -3.467e-02,  1.147e-02,  6.299e-02,  1.895e-01, -2.061e-01,\n",
       "       -1.611e-01, -4.468e-02,  3.467e-02,  1.074e-01,  4.336e-01,\n",
       "       -2.119e-01,  6.885e-02, -2.871e-01, -1.562e-01, -3.203e-01,\n",
       "        7.617e-02,  2.197e-02,  1.699e-01, -1.631e-01, -3.735e-02,\n",
       "        6.689e-02, -6.885e-02,  1.250e-01,  2.695e-01, -1.699e-01,\n",
       "       -2.031e-01, -2.344e-01,  3.320e-02, -6.299e-02,  2.061e-01,\n",
       "       -3.320e-01,  5.859e-02,  3.164e-01, -2.539e-02,  1.738e-01,\n",
       "        9.094e-03, -9.961e-02,  2.158e-01, -3.174e-02,  8.545e-02,\n",
       "       -1.191e-01, -3.574e-01,  3.340e-01, -8.740e-02, -7.520e-02,\n",
       "        7.373e-02, -8.984e-02, -1.206e-01, -2.051e-01, -5.127e-02,\n",
       "        2.910e-01,  2.773e-01,  1.445e-01,  4.238e-01, -2.413e-04],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 35
=======
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.32031250e-02, -4.98046875e-01,  6.12792969e-02,  3.30078125e-01,\n",
       "        2.19726562e-01, -2.46093750e-01,  1.03515625e-01,  5.78613281e-02,\n",
       "       -2.48046875e-01, -2.03125000e-01,  5.51757812e-02,  1.64062500e-01,\n",
       "        4.34570312e-02, -5.54687500e-01,  1.45507812e-01, -1.32812500e-01,\n",
       "        2.26562500e-01,  3.71093750e-02, -4.66308594e-02, -1.43554688e-01,\n",
       "       -1.39648438e-01,  7.27539062e-02, -2.04101562e-01, -1.04492188e-01,\n",
       "        5.24902344e-02,  1.17187500e-01, -4.76562500e-01, -1.44653320e-02,\n",
       "        2.69531250e-01, -2.55859375e-01, -5.49316406e-02,  2.41210938e-01,\n",
       "       -1.62109375e-01,  1.51367188e-01,  9.71679688e-02, -2.92968750e-01,\n",
       "       -6.44531250e-02,  2.67578125e-01,  7.76367188e-02, -5.42968750e-01,\n",
       "        1.04003906e-01,  1.97265625e-01,  5.49316406e-02,  6.22558594e-02,\n",
       "        1.04003906e-01, -3.49609375e-01, -1.95312500e-01,  1.40625000e-01,\n",
       "       -5.79833984e-03, -1.26953125e-01,  4.17480469e-02, -3.43750000e-01,\n",
       "       -3.26171875e-01,  6.22558594e-02, -6.01562500e-01,  2.21679688e-01,\n",
       "       -2.87109375e-01, -2.71484375e-01, -1.50390625e-01, -3.69140625e-01,\n",
       "        1.42578125e-01,  2.32421875e-01,  3.75976562e-02,  2.92968750e-01,\n",
       "       -3.84765625e-01, -3.80859375e-02, -1.24023438e-01, -2.55859375e-01,\n",
       "       -8.11767578e-03, -5.85937500e-02, -1.14746094e-01,  2.51953125e-01,\n",
       "       -4.05273438e-02, -2.51953125e-01,  2.03125000e-01, -1.02539062e-01,\n",
       "        1.66992188e-01,  8.00781250e-02,  1.33789062e-01,  3.69140625e-01,\n",
       "       -3.63281250e-01, -4.46777344e-02, -9.08203125e-02,  2.14843750e-01,\n",
       "        3.90625000e-01, -2.36328125e-01, -3.32031250e-01,  2.13867188e-01,\n",
       "        1.18652344e-01,  3.24707031e-02, -7.95898438e-02, -6.59179688e-02,\n",
       "       -1.14746094e-01, -2.45117188e-01, -3.33984375e-01, -3.37890625e-01,\n",
       "        2.21679688e-01, -1.71875000e-01,  1.01562500e-01, -4.90234375e-01,\n",
       "        1.20117188e-01, -2.71484375e-01, -1.54296875e-01, -5.34667969e-02,\n",
       "        6.68945312e-02, -2.67578125e-01,  1.68945312e-01, -4.02832031e-02,\n",
       "       -8.78906250e-02,  1.25976562e-01, -3.43750000e-01,  1.93359375e-01,\n",
       "        3.76953125e-01,  1.45507812e-01, -2.27539062e-01, -1.68945312e-01,\n",
       "        1.85546875e-01, -1.30859375e-01, -2.04101562e-01, -1.63085938e-01,\n",
       "        3.71093750e-02,  2.46093750e-01, -1.83593750e-01, -2.55859375e-01,\n",
       "        3.39843750e-01, -3.88183594e-02,  3.88183594e-02,  1.29882812e-01,\n",
       "       -1.86523438e-01,  9.71679688e-02,  1.96289062e-01,  3.95507812e-02,\n",
       "       -5.11718750e-01,  1.18164062e-01,  9.81445312e-02,  3.20312500e-01,\n",
       "        1.03027344e-01, -1.73339844e-02,  2.69531250e-01,  1.79687500e-01,\n",
       "        1.01074219e-01,  4.06250000e-01, -4.10156250e-02,  7.12890625e-02,\n",
       "        1.08886719e-01,  1.27563477e-02,  1.09863281e-02, -1.43554688e-01,\n",
       "        2.59765625e-01,  1.07421875e-01,  2.51953125e-01,  1.40625000e-01,\n",
       "       -3.28125000e-01,  1.57226562e-01,  1.87500000e-01, -1.42578125e-01,\n",
       "        1.99218750e-01,  1.28906250e-01,  6.22558594e-02, -2.92968750e-01,\n",
       "        1.14746094e-01,  1.39160156e-02, -8.34960938e-02, -3.00781250e-01,\n",
       "        2.67578125e-01,  8.74023438e-02, -1.38671875e-01,  3.67187500e-01,\n",
       "       -1.63574219e-02,  2.51953125e-01, -3.26171875e-01, -7.32421875e-02,\n",
       "        6.49414062e-02, -4.23828125e-01,  3.67187500e-01,  3.54003906e-03,\n",
       "       -1.92871094e-02,  6.00585938e-02, -2.81982422e-02, -9.71679688e-02,\n",
       "        4.49218750e-02, -1.51367188e-01,  2.23632812e-01, -2.53906250e-01,\n",
       "       -1.26953125e-01, -1.43554688e-01, -5.05371094e-02, -4.07714844e-02,\n",
       "       -3.71093750e-02,  1.60156250e-01, -2.53906250e-01, -2.63671875e-01,\n",
       "       -1.67968750e-01, -3.30078125e-01,  2.49023438e-01, -4.06250000e-01,\n",
       "       -5.63964844e-02,  2.20703125e-01,  2.63671875e-01,  3.56445312e-02,\n",
       "       -6.34765625e-02,  6.16455078e-03,  3.90625000e-01, -3.10546875e-01,\n",
       "       -2.32421875e-01,  2.06054688e-01,  1.89453125e-01, -1.71875000e-01,\n",
       "        1.45874023e-02, -6.73828125e-02,  4.76074219e-02, -9.13085938e-02,\n",
       "        4.44335938e-02,  1.63085938e-01, -2.49023438e-01,  2.46093750e-01,\n",
       "        8.15429688e-02,  7.56835938e-02, -5.07812500e-02, -4.25781250e-01,\n",
       "       -2.81250000e-01,  1.67968750e-01, -3.61328125e-02, -8.00781250e-02,\n",
       "       -1.64062500e-01,  6.93359375e-02,  3.20312500e-01, -4.12597656e-02,\n",
       "        1.67236328e-02,  1.90429688e-01,  3.37890625e-01, -3.06640625e-01,\n",
       "       -5.27343750e-01,  1.60156250e-01, -2.65625000e-01, -3.71093750e-01,\n",
       "        9.57031250e-02,  6.07910156e-02,  2.50000000e-01,  1.31835938e-01,\n",
       "        1.86523438e-01, -2.36328125e-01, -1.52343750e-01, -4.19921875e-01,\n",
       "        1.06445312e-01, -3.46679688e-02,  1.14746094e-02,  6.29882812e-02,\n",
       "        1.89453125e-01, -2.06054688e-01, -1.61132812e-01, -4.46777344e-02,\n",
       "        3.46679688e-02,  1.07421875e-01,  4.33593750e-01, -2.11914062e-01,\n",
       "        6.88476562e-02, -2.87109375e-01, -1.56250000e-01, -3.20312500e-01,\n",
       "        7.61718750e-02,  2.19726562e-02,  1.69921875e-01, -1.63085938e-01,\n",
       "       -3.73535156e-02,  6.68945312e-02, -6.88476562e-02,  1.25000000e-01,\n",
       "        2.69531250e-01, -1.69921875e-01, -2.03125000e-01, -2.34375000e-01,\n",
       "        3.32031250e-02, -6.29882812e-02,  2.06054688e-01, -3.32031250e-01,\n",
       "        5.85937500e-02,  3.16406250e-01, -2.53906250e-02,  1.73828125e-01,\n",
       "        9.09423828e-03, -9.96093750e-02,  2.15820312e-01, -3.17382812e-02,\n",
       "        8.54492188e-02, -1.19140625e-01, -3.57421875e-01,  3.33984375e-01,\n",
       "       -8.74023438e-02, -7.51953125e-02,  7.37304688e-02, -8.98437500e-02,\n",
       "       -1.20605469e-01, -2.05078125e-01, -5.12695312e-02,  2.91015625e-01,\n",
       "        2.77343750e-01,  1.44531250e-01,  4.23828125e-01, -2.41279602e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    }
   ],
   "source": [
    "# We can access individual word vectors using a dictionary-like syntax\n",
    "model['Metis']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
     "data": {
      "text/plain": [
       "[('meetings', 0.757283091545105),\n",
       " ('meeeting', 0.6875465512275696),\n",
       " ('Meeting', 0.6848756074905396),\n",
<<<<<<< HEAD
       " ('meeing', 0.6018308997154236),\n",
       " ('Meetings', 0.5899163484573364),\n",
       " ('worksession', 0.5894558429718018),\n",
       " ('roundtable', 0.5851958990097046),\n",
       " ('briefing', 0.5807524919509888)]"
      ]
     },
     "metadata": {},
     "execution_count": 36
=======
       " ('meeing', 0.6018308401107788),\n",
       " ('Meetings', 0.5899163484573364),\n",
       " ('worksession', 0.589455783367157),\n",
       " ('roundtable', 0.5851958990097046),\n",
       " ('briefing', 0.5807525515556335)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    }
   ],
   "source": [
    "# Some cool results!\n",
    "\n",
    "model.most_similar('meeting' ,topn=8)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('President', 0.8006276488304138),\n",
       " ('chairman', 0.6708744764328003),\n",
       " ('vice_president', 0.6700225472450256),\n",
       " ('chief_executive', 0.6691274642944336),\n",
       " ('CEO', 0.6590125560760498)]"
      ]
     },
     "metadata": {},
     "execution_count": 37
=======
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('President', 0.800627589225769),\n",
       " ('chairman', 0.6708744764328003),\n",
       " ('vice_president', 0.6700225472450256),\n",
       " ('chief_executive', 0.6691275238990784),\n",
       " ('CEO', 0.6590125560760498)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    }
   ],
   "source": [
    "model.most_similar('president' ,topn=5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('queen', 0.7118192315101624),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
=======
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431607246399),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
<<<<<<< HEAD
       " ('monarchy', 0.5087411403656006)]"
      ]
     },
     "metadata": {},
     "execution_count": 29
=======
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    }
   ],
   "source": [
    "# Here's an analogy task!\n",
    "\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
=======
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
     "data": {
      "text/plain": [
       "[('France', 0.667637825012207),\n",
       " ('Les_Bleus', 0.5665801167488098),\n",
       " ('Stade_De', 0.5045602917671204),\n",
<<<<<<< HEAD
       " ('Marseille', 0.5020229816436768),\n",
       " ('Marc_Lièvremont', 0.500834584236145),\n",
       " ('les_bleus', 0.49737778306007385),\n",
       " ('Les_Tricolores', 0.49725979566574097),\n",
       " ('Fabien_Galthié', 0.4901474714279175),\n",
       " ('French', 0.4892624616622925),\n",
       " ('les_Bleus', 0.4850963056087494)]"
      ]
     },
     "metadata": {},
     "execution_count": 30
=======
       " ('Marseille', 0.502022922039032),\n",
       " ('Marc_Lièvremont', 0.500834584236145),\n",
       " ('les_bleus', 0.49737775325775146),\n",
       " ('Les_Tricolores', 0.4972597658634186),\n",
       " ('Fabien_Galthié', 0.4901474714279175),\n",
       " ('French', 0.4892624318599701),\n",
       " ('les_Bleus', 0.48509636521339417)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    }
   ],
   "source": [
    "model.most_similar(positive=['Paris', 'England'], negative=['London'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GloVe with Gensim requires a little extra leg work, but it's not too bad.\n",
    "The problem is that the file format that is publicly available doesn't play nice with Gensim.\n",
    "Luckily, Gensim provides a handy method of converting it!"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(400001, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "glove_file = glove_dir = '/Users/ridwan/Documents/DataScience/Metis/Metis_Projects/Data/word_embeddings/glove.6B/glove.6B.100d.txt'\n",
    "\n",
    "w2v_output_file = '/Users/ridwan/Documents/DataScience/Metis/Metis_Projects/Data/word_embeddings/glove.6B/glove.6B.100d.txt.w2v'\n",
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘glove/glove.6B.zip’ already there; not retrieving.\n",
      "\n",
      "Archive:  glove/glove.6B.zip\n",
      "  inflating: glove/glove.6B.50d.txt  \n",
      "  inflating: glove/glove.6B.100d.txt  \n",
      "  inflating: glove/glove.6B.200d.txt  \n",
      "  inflating: glove/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "# Load the Google vectors\n",
    "! mkdir -p glove\n",
    "! wget -nc -P glove http://nlp.stanford.edu/data/glove.6B.zip\n",
    "! unzip -d glove -o glove/glove.6B\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    "\n",
    "glove_file = 'glove/glove.6B.50d.txt'\n",
    "tmp_file = get_tmpfile(\"glove_word2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 39,
=======
   "execution_count": 2,
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
   "metadata": {},
   "outputs": [],
   "source": [
    "# call glove2word2vec script\n",
    "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('conference', 0.8648155331611633),\n",
       " ('meetings', 0.8619149923324585),\n",
       " ('summit', 0.8164560794830322),\n",
       " ('talks', 0.814795970916748),\n",
       " ('discuss', 0.795186460018158),\n",
       " ('ministers', 0.7842442989349365),\n",
       " ('met', 0.7822972536087036),\n",
       " ('leaders', 0.7725202441215515)]"
      ]
     },
     "metadata": {},
     "execution_count": 40
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conference', 0.9094429016113281),\n",
       " ('meetings', 0.8820489645004272),\n",
       " ('talks', 0.8670185804367065),\n",
       " ('discuss', 0.8639705181121826),\n",
       " ('meet', 0.8616299629211426),\n",
       " ('summit', 0.8573991060256958),\n",
       " ('ministers', 0.8558546900749207),\n",
       " ('delegation', 0.8443616628646851)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    }
   ],
   "source": [
    "# How does it compare to the previous examples we did with word2vec?\n",
    "model.most_similar('meeting' ,topn=8)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('vice', 0.828760027885437),\n",
       " ('presidency', 0.7150216102600098),\n",
       " ('former', 0.706093966960907),\n",
       " ('presidents', 0.6961983442306519),\n",
       " ('chairman', 0.6928698420524597)]"
      ]
     },
     "metadata": {},
     "execution_count": 41
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vice', 0.8557188510894775),\n",
       " ('met', 0.8169845342636108),\n",
       " ('secretary', 0.8095138072967529),\n",
       " ('presidency', 0.7797858119010925),\n",
       " ('chairman', 0.7695391178131104)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    }
   ],
   "source": [
    "model.most_similar('president' ,topn=5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483),\n",
       " ('monarch', 0.6843380928039551),\n",
       " ('throne', 0.6755736470222473),\n",
       " ('daughter', 0.6594556570053101),\n",
       " ('princess', 0.6520534157752991),\n",
       " ('prince', 0.6517034769058228),\n",
       " ('elizabeth', 0.6464518308639526),\n",
       " ('mother', 0.6311717629432678),\n",
       " ('emperor', 0.6106470823287964),\n",
       " ('wife', 0.6098655462265015)]"
      ]
     },
     "metadata": {},
     "execution_count": 42
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8523603677749634),\n",
       " ('throne', 0.7664333581924438),\n",
       " ('prince', 0.7592144012451172),\n",
       " ('daughter', 0.7473883032798767),\n",
       " ('elizabeth', 0.7460219860076904),\n",
       " ('princess', 0.7424570322036743),\n",
       " ('kingdom', 0.7337411642074585),\n",
       " ('monarch', 0.721449077129364),\n",
       " ('eldest', 0.7184862494468689),\n",
       " ('widow', 0.7099430561065674)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn!\n",
    "\n",
    "- Using either word2vec or GloVe, what interesting analogies or relationships?\n",
    "\n",
    "- Given a short piece of text (like a tweet) what strategies can you think of to create a \"tweet vector\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
=======
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
>>>>>>> 83ad4b74267aa8a79dbda03f35f34a69a6cd4848
